# Beyond Accuracy: Grounding Evaluation Metrics for Human-Machine Learning Systems

### Fernando Diaz, Brian St. Thomas, Praveen Chandar - Presented at  [NeurIPS2020](https://nips.cc/virtual/2020/public/e_tutorials.html)
 

-----
## Abstract
The evaluation and optimization of machine learning systems have largely adopted
well-known performance metrics like accuracy (for classification) or squared error
(for regression). While these metrics are reusable across a variety of machine
learning tasks, they make strong assumptions often not observed when situated in a
broader technical or sociotechnical system. This is especially true in systems that
interact with large populations of humans attempting to complete a task or satisfy a
need (e.g. search, recommendation, game-playing). In this tutorial, we will present
methods for developing evaluation metrics grounded in what users expect of the
system and they respond to system decisions. The goal of this tutorial is both to
share methods for designing user-based quantitative metrics and to motivate new
research into optimizing for these more structured metrics.


# Slides

- Part 1: [Introduction](./neurips_2020/intro.pdf)
- Part 2: [Offline Metrics](./neurips_2020/offline_metrics.pdf)
- Part 3: [Behavioral Metrics](./neurips_2020/online_metrics.pdf) 
- Part 4: [Multiple Metrics](./neurips_2020/multiple_metrics.pdf) 